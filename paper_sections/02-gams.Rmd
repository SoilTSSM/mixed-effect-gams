
#II: an introduction to Generalized Additive Models

One of the most common model formulations in statistics is the generalized linear model [@McCullagh:1989ti] that relates a response ($y$) to a linear combination of explanatory variables. The response is assumed to be conditionally distributed according to a member of the exponential family (e.g., letting the response be a trial, a count or a strictly positive real number --- binomial, Poisson or Gamma distributions, respectively). The generalized additive model [GAM; @Hastie:1990vg; @Ruppert:2003uc; @Wood:2006vg] allows the relationships between the explanatory variables (henceforth covariates) and the response to be described by smooth terms (usually *splines* [@deBoor:1978wq], but potentially other structures). In general we have models of the form:
$$
\mathbb{E}\left( y \right) = g^{-1}\left( \beta_0 + \sum_{j=1}^J f_j(x_j) \right)\,,
$$
where $\mathbb{E}(y)$ is the expected value of the response $y$ (with an appropriate distribution and link function $g$), $f_j$ is a smooth function of the covariate $x_j$, $\beta_0$ is an intercept term and $g^{-1}$ is the inverse link function. Here there are $J$ smooths and each is a function of only one covariate, though it is possible to construct smooths of multiple variables.

Each smooth $f_j$ is represented by a sum of simpler *basis functions* ($b_k$) multiplied by corresponding coefficients ($\beta_k$), which need to be estimated:
$$
f_j(x_j) = \sum_{k=1}^K \beta_k b_k(x_j)\,,
$$
The size $K$ of each smooth will determine the complexity of the resulting term (referred to as "basis size", "basis complexity" or "basis richness"). Though it seems like the basis can be overly complex ("how big should I make $K$?") and lead to overfitting, we need not worry about this as we use a penalty to ensure that each function's complexity is appropriate; hence the basis only need be *large enough* and we let the penalty deal with excess wigglyness.

The penalty for a term is usually based on a derivative of the smooth, as the derivative is used to measure the wigglyness of the function and hence its flexibility. Model fit is measured by its likelihood ($\mathcal{L}(y|\theta)$, the likelihood of observing the data, $y$, given the parameters $\theta$). We trade-off the fit of the model against the wigglyness penalty to obtain a model that fits the data well but does not overfit. To control this trade-off we estimate a *smoothness parameter* for each smooth $\lambda_j$. This trade-off results in the penalised likelihood $\mathcal{L}_p$ of the estimated model
$$
\mathcal{L}_p = \underbrace{\mathcal{L}(y | \theta)}_{\text{model fit}} + \lambda \underbrace{\beta^{\mathsf{T}}\mathbf{S}\beta}_{\makebox[0pt]{\text{{\scriptsize wiggliness}}}}.
$$

Figure \ref{fig:smoothing_effect} shows optimal smoothing (where the smoothing parameter is estimated to give a parsimonious model) in the first plot; the second plot shows what happens when the smoothing parameter is set to zero, so the penalty has no effect (interpolation); the right plot shows when the smoothing parameter is set to a very large value, giving a straight line. Smooths of this kind are often referred to as a *basis-penalty smoothers*.

**say something about knots!**

**GLS: Given that I just dropped in the quadratic form of the penalty do we want to explain this, mention $\mathbf{S}$, etc?**

**say something about penalty matrices and also smoothing parameters**

```{r lambda, echo=FALSE, message=FALSE, results='hide', fig.width=8, fig.height=2.5, cache=TRUE, fig.cap="\\label{fig:smoothing_effect}Examples of how different choices of the smoothing parameter effect the resulting function. Data (points) were generated from the blue function and noise added to them. In the left plot the smoothing parameter was estimated to give a good fit to the data, in the middle plot the smoothing parameter was set to zero, so the penalty has no effect and the function interpolates the data, the right plot shows when the smoothing parameter is set to a very large value, so the penalty removes all terms that have any wigglyness, giving a straight line. Numbers in the $y$ axis labels show the estimated degrees of freedom for the term.", messages=FALSE, dev=c('pdf'), out.width="\\linewidth"}
# example of varying lambda

library(mgcv)
library("ggplot2")
library("cowplot")
theme_set(theme_bw())

set.seed(12)

# generate some data
dat <- gamSim(1, n=100, dist="normal", scale=2)
dat$y <- dat$y - (dat$f1 + dat$f0 + dat$f3)
dat$x <- dat$x2
true <- data.frame(x = sort(dat$x),
                   y = dat$f2[order(dat$x)])

## GCV fit
b <- gam(y~s(x, k=100), data=dat)

# lambda=0
b.0 <- gam(y~s(x, k=100), data=dat, sp=0)

# lambda=infinity
b.inf <- gam(y~s(x, k=100), data=dat, sp=1e10)

pdat <- with(dat, data.frame(x = seq(min(x), max(x), length = 200)))
p <- cbind(pdat, fit = predict(b, newdata = pdat))
p.0 <- cbind(pdat, fit = predict(b.0, newdata = pdat))
p.inf <- cbind(pdat, fit = predict(b.inf, newdata = pdat))
ylims <- range(p, p.0, p.inf)

lab.l <- labs(x = "x", y = "y")
dat.l <- geom_point(data = dat, aes(x = x, y = y), colour = "darkgrey")
true.l <- geom_line(data = true, aes(x = x, y = y), colour = "blue")
coord.l <- coord_cartesian(ylim = ylims)

p1 <- ggplot(p, aes(x = x, y = fit)) +
  dat.l + true.l +
  geom_line(colour = "darkred") + lab.l + coord.l

p2 <- ggplot(p.0, aes(x = x, y = fit)) +
  dat.l + true.l +
  geom_line(colour = "darkred") + lab.l + coord.l

p3 <- ggplot(p.inf, aes(x = x, y = fit)) +
  dat.l + true.l +
  geom_line(colour = "darkred") + lab.l + coord.l

plot_grid(p1, p2, p3, align = "hv", axis = "lrtb", ncol = 3, labels = "auto")
```

The number of basis functions, $K$, limits the maximum complexity for a given smooth term. To measure the complexity of a smooth term, we use the *effective degrees of freedom* (EDF), which at a maximum is the number of coefficients to be estimated in the model, minus any constraints. The EDF can take non-integer values and larger values indicate more wiggly terms. See Wood [-@Wood:2006vg, Section 4.4] for further details.

There are many possible basis functions that can be used to construct the $b_k$. Here we'll use thin plate regression splines (TPRS), which have the appealing property that knot choice is somewhat automatic (the best approximation to including knots at each data point is used; [@wood_thin_2003]).

**DLM:: put the cubic splines back in here**

TPRS are also defined for any number of predictors, so multivariate smoothers can be constructed easily. The basis is *isotropic*; smoothness is treated the same in all directions. If one wanted a bivariate smooth of temperature and time a one degree change in temperature would equate to a one second change in time with an isotropic smooth, which is an odd assumption to make. In the more general case where units are not alike, we can use *tensor products* to combine two or more univariate marginal smooths into a more complex basis. Each marginal smooth can use a different type of basis, as wanted or dictated by the data[^1]

[^1]: e.g.\ a cyclic smooth basis for week of year or site aspect, where we wish to constrain the end points of the range of the covariate to be equal.

In the linear modelling literature we can specify a single interaction between terms (in R, `a:b`) or a "full interaction", which includes the marginal terms (`a*b` in R, which is equivalent to `a + b + a:b`). There are parallels for smooths too, allowing us to separate-out the main effect terms from the interactions (in R `te(a, b)` specifies the tensor product which is equivalent to `ti(a) + ti(b) + ti(a, b)`). The ability to separate out the interactions and main effects will become very useful in the next section, once we start looking at group-level smooths. For an example of a tensor product interaction, see figure \ref{mackerel_ti_exp}, which illustrates marginal smooths and a tensor product interaction of depth and salinity used to estimated expected mackerel egg counts from a marine survey.

```{r mackerel-tensor, echo=FALSE, results='hide', fig.width=5, fig.height=5, messages=FALSE, dev=c('pdf'), cache=TRUE, fig.cap="\\label{mackerel_ti_exp}Tensor product of depth and salinity with data taken from a 1992 survey of mackerel eggs. The two left plots show the marginal smooths of each term in the model (`ti(s.depth)` above and `ti(salinity)` below), the right plot shows the interaction effect (`ti(s.depth, salinity)`). Data are from @Wood:2006vg."}
library(mgcv)
library(gamair)

# use the mackerel data example
data(mack)

b <- gam(egg.count~ti(s.depth) + ti(salinity) + ti(s.depth, salinity),
         family=tw(), data=mack)

layout(matrix(c(1,2,3,3), 2, 2), widths=c(1.5,1.5,1), height=c(1,1,2))
par(mar=c(4, 3, 1, 2) + 0.1)
plot(b, select=1)
plot(b, select=2)
par(mar=c(0, 0, 0, 0) + 0.1)
vis.gam(b, view=c("s.depth","salinity"), theta=-60, phi=30, color="bw")
```

**GLS: The first sentence below is quite repetitive. That Bayesian bit comes out of nowhere here, even with the repetitive intro. Likewise the bit above about the interactions seems odd, and the analogy of `A*B` == `te(A, B)` vs `A + B + A:B` == `ti(A) + ti(B) + ti(A, B)` is not right; `A*B` is a syntactic shortcut for the marginal effects plus the interaction --- it decomposes into 3 separate terms unlike `te()`, and all the `ti()`s lead to extra penalties over the `te()` version.**

*We represent the terms in our model as basis functions, which end up as additional columns in our design matrix and parameter vector, and penalties, which penalize the likelihood and stop our model from being too wiggly.* Taking a pragmatic Bayesian approach to the problem, the penalty is really a prior on how we think the model should act. In which case the penalty matrix itself is a prior precision matrix (inverse variance) for the term. With that in mind, we can think about random effects as "smooths" in our model, albeit ones with ridge penalties [@kimeldorf_correspondence_1970; @wood_generalized_2017]. For instance, to include a random effect to account for variation in group means (intercepts) there will be one basis function for each level of the grouping variable, that takes a value of 1 for any observation in that group and 0 for any observation not in the group. The penalty matrix for these terms is a $n_g$ by $n_g$ identity matrix, where $n_g$ is the number of groups. This means that each group-level coefficient will be penalized in proportion to its squared deviation from zero. This is equivilent to how random effects are estimated in standard mixed effect models. The penalty term here is proportional to the inverse of the variance of the fixed effect estimated by standard hierarchical model solvers [@verbyla_analysis_2002  does this contradict what's above??]. 

This connection between random effects and basis function smooths extends beyond the varying-intercept case. Any basis-function representation of a smooth can be transformed so that it can be represented as a combination of a random effect with an associated variance, and possibly one or more fixed effects, corresponding to functions in the null space of the original basis-function (see below). While this is beyond the scope of this paper, see @verbyla_analysis_2002 or @wood_straightforward_2012 for a more detailed discussion on the connections between these approaches.

### Smoothing penalties vs. shrinkage penalties

**does this go above??**

**EJP: I think this makes sense here**

Penalties can have two effects on how well a model fits: they can penalize how wiggly a given term is (smoothing) and they can penalize the absolute size of the function (shrinkage). The penalty can only effect the components of the smooth that have derivatives (the *range space*), not the other parts (the *null space*). For 1-dimensional thin plate regression splines, this means that there is a linear term left in the model, even when the penalty is in full force (as $\lambda \rightarrow \infty$), as shown in figure \ref{fig:basis_example}^[This is also why figure\ \ref{fig:smoothing_effect}c resulted in a linear fit to the data.] **BLAH**. It is often useful to be able to remove null space functions as well, to be able to shrink them to zero if they do not contribute significantly to a given model fit. This can be done either by tweaking the penalty matrix so that it both smooths and shrinks as the single penalty term increases, or by adding a new penalty term that just penalizes the null space for the model **REF: Marra & Wood 2011**. Figure \ref{fig:basis_example} shows an example of what the basis functions (Fig. \ref{fig:basis_example}A), and smoothing penalties and shrinkage penalties (Fig. \ref{fig:basis_example}B) look like for a 6-basis function cubic spline and for a 6-basis function thin-plate spline. The random effects smoother we discussed earlier is an example of a pure shrinkage penalty; it penalizes all deviations away from zero, no matter the pattern of those deviations. This will come in useful later in section III, where we use random effect smoothers as one of the components of a HGAM.


**DLM: say something about cs basis here**

**EDIT this figure**

**Should this figure include the intercept basis function as well for each? 
Right now I've excluded as mgcv drops it automatically, but it is part of the 
basis, and comes into play with tensor products...**

**Do we actually need the cubic spline here? It would simplify the presentation to 
just show the thin plate spline, and it gets the idea across.**

**GLS: we've glossed over exactly how TPRS bases are created, the low-rank version of Wood, nothing on knots...**

![\label{fig:basis_example}a) Examples of the basis functions associated with a six basis function thin plate spline (top) and a cubic spline (bottom) calculated for x data spread evenly between $x=0$ and $x=1$. Each line represents a single basis function. To generate a given smooth function, each basis function would be multiplied by its own coefficient. b) The smoothing and shinkage penalty matrices for the thin plate and cubic smoothers shown on left. Red entries indicate positive values and blue indicate negative values. For example, for the thin plate spline, functions F3 and F4 would have the greatest proportionate effect on the total penalty (as they have the largest values on the diagonal), whereas function F6 would not contribute to the wiggliness penalty at all (all the values in the 6th row and column of the penalty matrix are zero). This means function F6 is in the null space of this basis, and is completely smooth.](../figures/example spline basis and penalties.png)

## Comparison to hierarchical linear models

Generalized linear mixed effect models [GLMMs; also referred to as hiearchical generalized linear models, multilevel models etc; e.g., @Bolker:2009cs; @Gelman:2006jh] are an extension of regression modelling that allow the modeller to include terms in the model that account for structure in the data --- the structure is usually of the form of a nesting of the observations. For example individuals are nested within sample sites, sites are nested within forests and forests within states. The depth of the nesting is limited by the fitting procedure and number of parameters to estimate.

HGLMs are a highly flexible way to think about grouping in data; the groupings used in models often refer to the spatial or temporal scale of the data [@McMahon:2007ju] though can be based on any useful grouping.

We would like to be able to think about the groupings in our data in a simple way, even when the covariates in our model are related to the response in a non-linear way. The next section investigates the extension of the smoothers we showed above to the case where each observation is in a group, with a group-level smooth.


**This last section should say something like: these are both latent gaussian thingos, the model structure is the difference --- we want to be able to heirarchically structure our smoothers**
