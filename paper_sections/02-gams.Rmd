
#II: A review of Generalized Additive Models

The generalized linear model [GLM; @McCullagh:1989ti] relates a response ($y$) to a linear combination of explanatory variables. The response is assumed to be conditionally distributed according to some exponential family distribution (e.g., binomial, Poisson or Gamma distributions for trial, count or strictly positive real response, respectively). The generalized additive model [GAM; @Hastie:1990vg; @Ruppert:2003uc; @wood_generalized_2017] allows the relationships between the explanatory variables (henceforth covariates) and the response to be described by smooth terms (usually *splines* [@deBoor:1978wq], but potentially other structures). In general we have models of the form:
$$
\mathbb{E}\left( Y \right) = g^{-1}\left( \beta_0 + \sum_{j=1}^J f_j(x_j) \right)\,,
$$
where $\mathbb{E}(Y)$ is the expected value of the response $Y$ (with an appropriate distribution and link function $g$), $f_j$ is a smooth function of the covariate $x_j$, $\beta_0$ is an intercept term and $g^{-1}$ is the inverse link function. Here there are $J$ smooths and each is a function of only one covariate in this example, though it is possible to construct smooths of multiple variables.

Each smooth $f_j$ is represented by a sum of $K$ simpler, fixed *basis functions* ($b_{j,k}$) multiplied by corresponding coefficients ($\beta_{j,k}$), which need to be estimated:
$$
f_j(x_j) = \sum_{k=1}^K \beta_{j,k} b_{j,k}(x_j).
$$
$K$, referred to as "basis size", "basis complexity" or "basis richness", determines the maximum complexity of each smooth.

It would seem that large basis size could lead to overfitting, but this counteracted by a *smoothing penalty* that influences basis function coefficients so as to prevent excess wiggliness and ensure appropriate complexity of each smooth term.  For each term, a *penalty matrix* $(\mathbf{S})$, specific to the form of the basis functions, is pre- and post-multiplied by the parameter vector $\boldsymbol{\beta}$ to calculate the penalty $(\boldsymbol{\beta}^T \mathbf{S} \boldsymbol{\beta})$.  We then penalize the model likelihood by the penalty term, controlling the trade-off via a *smoothing parameter* ($\lambda$).  The penalized likelihood used to fit the model is thus
$$
L - \boldsymbol{\lambda} \boldsymbol{\beta}^T \mathbf{S} \boldsymbol{\beta}
$$

Figure \ref{fig:smoothing_effect} shows the results of setting different smoothing parameters for a simple one-dimensional smoothing problem: optimal smoothing in the first plot; the second plot shows what happens when the smoothing parameter is set to zero: interpolation (the penalty has no effect); the right plot shows when the smoothing parameter is set to a very large value, resulting in a straight line.

To measure the complexity of an penalized smooth terms we use the *effective degrees of freedom* (EDF), which at a maximum is the number of coefficients to be estimated in the model, minus any constraints. The EDF can take non-integer values and larger values indicate more wiggly terms (see Wood [-@wood_generalized_2017, Section 6.1.2] for further details).

```{r lambda, echo=FALSE, message=FALSE, results='hide', fig.width=8, fig.height=2.5, cache=TRUE, fig.cap="\\label{fig:smoothing_effect}Examples of how different choices of the smoothing parameter effect the resulting function. Data (points) were generated from the blue function and noise added to them. In the left plot the smoothing parameter was estimated using REstricted Maximum Likelihood to give a good fit to the data, in the middle plot the smoothing parameter was set to zero, so the penalty has no effect and the function interpolates the data, the right plot shows when the smoothing parameter is set to a very large value, so the penalty removes all terms that have any wiggliness, giving a straight line.", messages=FALSE, dev=c('pdf'), out.width="\\linewidth"}
# example of varying lambda

set.seed(12)

# generate some data
dat <- gamSim(1, n=100, dist="normal", scale=2)
dat$y <- dat$y - (dat$f1 + dat$f0 + dat$f3)
dat$x <- dat$x2
true <- data.frame(x = sort(dat$x),
                   y = dat$f2[order(dat$x)])

## REML fit
b <- gam(y~s(x, k=100), data=dat, method = "REML")

# lambda=0
b.0 <- gam(y~s(x, k=100), data=dat, sp=0)

# lambda=infinity
b.inf <- gam(y~s(x, k=100), data=dat, sp=1e10)

pdat <- with(dat, data.frame(x = seq(min(x), max(x), length = 200)))
p <- cbind(pdat, fit = predict(b, newdata = pdat))
p.0 <- cbind(pdat, fit = predict(b.0, newdata = pdat))
p.inf <- cbind(pdat, fit = predict(b.inf, newdata = pdat))
ylims <- range(p, p.0, p.inf)

lab.l <- labs(x = "x", y = "y")
dat.l <- geom_point(data = dat, aes(x = x, y = y), colour = "darkgrey")
true.l <- geom_line(data = true, aes(x = x, y = y), colour = "blue")
coord.l <- coord_cartesian(ylim = ylims)

p1 <- ggplot(p, aes(x = x, y = fit)) +
  dat.l + true.l +
  geom_line(colour = "darkred") + lab.l + coord.l

p2 <- ggplot(p.0, aes(x = x, y = fit)) +
  dat.l + true.l +
  geom_line(colour = "darkred") + lab.l + coord.l

p3 <- ggplot(p.inf, aes(x = x, y = fit)) +
  dat.l + true.l +
  geom_line(colour = "darkred") + lab.l + coord.l

plot_grid(p1, p2, p3, align = "hv", axis = "lrtb", ncol = 3, labels = "auto")
```

### Basis types and penalty matrices

Different types of smooths are useful for different needs, and have different associated penalty matrices for their basis function coefficients.  In the examples in this paper, we will use three types of smooths: thin plate regression splines, cyclic cubic smooths, and random effects. 

Thin plate regression splines [TPRS; @wood_thin_2003], are a general purpose spline basis which can be use for problems in any number of dimensions, provided one can assume that the amount of smoothing in any of the covariates is the same (so called isotropy or rotational invariance).  TPRS, like many splines, use a penalty matrix made up of terms based on the the squared derivatives of basis functions across their range. Models that overfit the data will tend to have large derivatives, so this penalization reduces wiggliness. Typically, TPRS are second-order ($m=2$), meaning that squared second derivatives are used. However, TPRS may be of lower order ($m=1$), or higher order ($m > 2$).  We will see in section III how lower-order TPRS smooths are useful in fitting HGAMs. Example basis functions and penalty matrix $\mathbf{S}$ for a $m=2$ TPRS with six basis functions for evenly spaced data are shown in figure \ref{fig:basis_example}.

Cyclic cubic smoothers are another smoother that penalizes the squared second derivative of the smooth across the function. In these, though, start and end of the smoother are constrained to match in value and first derivative. These are useful for fitting models with cyclic components such as seasonal effects. We will use these smoothers to demonstrate how to fit HGAMs to cyclic data.

Random effects are also "smooths" in this framework. In this case, the penalty matrix is the inverse of the covariance matrix of the basis function coefficients [@kimeldorf_correspondence_1970; @wood_generalized_2017]. To include a simple single-level random effect to account for variation in group means (intercepts) there will be one basis function for each level of the grouping variable, that takes a value of 1 for any observation in that group and 0 for any observation not in the group. The penalty matrix for these terms is a $n_g$ by $n_g$ identity matrix, where $n_g$ is the number of groups. This means that each group-level coefficient will be penalized in proportion to its squared deviation from zero. This is equivalent to how random effects are estimated in standard mixed effect models. The penalty term is then proportional to the inverse of the variance of the fixed effect estimated by standard hierarchical model software [@verbyla_analysis_2002].

This connection between random effects and splines extends beyond the varying-intercept case. Any single-penalty basis-function representation of a smooth can be transformed so that it can be represented as a combination of a random effect with an associated variance, and possibly one or more fixed effects. See @verbyla_analysis_2002 or @wood_straightforward_2012 for a more detailed discussion on the connections between these approaches.

### Smoothing penalties vs. shrinkage penalties

Penalties can have two effects on how well a model fits: they can penalize how wiggly a given term is (smoothing) and they can penalize the absolute size of the function (shrinkage). The penalty can only effect the components of the smooth that have derivatives (the *range space*), not the other parts (the *null space*). For 1-dimensional thin plate regression splines (when  $m=2$), this means that there is a linear term left in the model, even when the penalty is in full force (as $\lambda \rightarrow \infty$), as shown in figure \ref{fig:basis_example} (this is also why figure\ \ref{fig:smoothing_effect}c resulted in a linear, rather than flat, fit to the data). The random effects smoother we discussed earlier is an example of a pure shrinkage penalty; it penalizes all deviations away from zero, no matter the pattern of those deviations. This will be useful later in section III, where we use random effect smoothers as one of the components of a HGAM.


![\label{fig:basis_example}a) Examples of the basis functions associated with a six basis function thin plate spline (m=2), calculated for data, $x$, spread evenly between $x=0$ and $x=1$. Each line represents a single basis function. b) The smoothing  penalty matrix for the thin plate smoother. Red entries indicate positive values and blue indicate negative values.  For example, functions F3 and F4 would have the greatest proportionate effect on the total penalty (as they have the largest values on the diagonal), whereas function F5 and F6 would not contribute to the wiggliness penalty at all (all the values in the 5th and 6th row and column of the penalty matrix are zero). This means these functions are in the null space of this basis, and are treated as completely smooth. c) An example of how the basis functions add up to create a single smooth function. Thin coloured lines represent each basis function multiplied by a coefficient, and the solid black line is the sum of those basis functions.](../figures/example_spline_basis_and_penalties.png)

## Interactions between smooth terms

It is also possible to create interactions between covariates with different smoothers (or degrees of smoothness) assumed for each covariate, using *tensor products*. For instance, if one wanted estimate the interacting effects of temperature and time (in seconds) on some outcome, it would not make sense to use a two-dimensional TPRS smoother, as that would assume that a one degree change in temperature would equate to a one second change in time. Instead, a tensor product allows us to create a new set of basis functions that allow for each marginal function (here temperature and time) to have its own marginal smoothness penalty. A different basis can be used in each marginal smooth, as required for the data at hand.

There are two approaches used in **mgcv** for generating tensor products. The first approach [@wood_lowrank_2006] essentially creates an interaction of each pair of basis functions for each marginal term, and a penalty for each marginal term that penalizes the average average wiggliness in that term; in **mgcv**, these are created using the `te()` function. The second approach [@wood_straightforward_2012] separates each penalty into penalized (range space) and unpenalized components (null space; components that don't have derivatives, such as intercept and linear terms in a one-dimensional cubic spline), then creates new basis functions and penalties for all pair-wise combinations of penalized and unpenalized components between all pairs of marginal bases; in **mgcv**, these are created using the `t2()` function. The advantage of the first method is that it requires fewer smoothing parameters, so is faster to estimate in most cases. The advantage of the second method is that the tensor products created this way only have a single penalty associated with each marginal basis (unlike the `te()` approach, where each penalty applies to all basis functions), so it can be fitted using standard mixed effect software such as **lme4** [@bates_fitting_2015].

## Comparison to hierarchical linear models

Generalized linear mixed effect models [GLMMs; also referred to as hierarchical generalized linear models, multilevel models etc; e.g., @Bolker:2009cs; @Gelman:2006jh] are an extension of regression modelling that allows the inclusion of terms in the model that account for structure in the data --- the structure is usually of the form of a nesting of the observations. For example, in an empirical study, individuals may be nested within sample sites, sites are nested within forests, and forests within provinces. The depth of the nesting is limited by the fitting procedure and number of parameters to estimate.

HGLMs are a highly flexible way to think about grouping in data; the groupings used in models often refer to the spatial or temporal scale of the data [@McMahon:2007ju] though can be based on any useful grouping.

We would like to be able to think about the groupings in our data in a similar way, even when the covariates in our model are related to the response in a smooth way. The next section investigates the extension of the smoothers we showed above to the case where observations are grouped and we model group-level smooths.
